{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BostonHousingDataset.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanyadixit21/GeneralModels/blob/master/BostonHousingDataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "MHw9xd9BXpc-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Regression on Boston Housing Data"
      ]
    },
    {
      "metadata": {
        "id": "I_qnYylPYBdB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "bfea4efb-1b54-4f92-c05e-3f1c91169fa6"
      },
      "cell_type": "code",
      "source": [
        "!pip install mglearn"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mglearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/01/8d3630ecc767c9de96a9c46e055f2a3a5f9e14a47d3d0348a36a5005fe67/mglearn-0.1.7.tar.gz (540kB)\n",
            "\u001b[K    100% |████████████████████████████████| 542kB 22.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mglearn) (1.16.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from mglearn) (3.0.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from mglearn) (0.20.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from mglearn) (0.23.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from mglearn) (4.3.0)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.6/dist-packages (from mglearn) (0.10.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from mglearn) (2.4.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mglearn) (2.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mglearn) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mglearn) (2.5.3)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->mglearn) (1.2.1)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->mglearn) (2018.9)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->mglearn) (0.46)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler->mglearn) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->mglearn) (40.9.0)\n",
            "Building wheels for collected packages: mglearn\n",
            "  Building wheel for mglearn (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/74/cf/8d/04f4932d15854a36726c6210763c7127e62de28f5c8ddfcf3b\n",
            "Successfully built mglearn\n",
            "Installing collected packages: mglearn\n",
            "Successfully installed mglearn-0.1.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ys0SW_OHXpdA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "import mglearn\n",
        "import numpy as np\n",
        "import mglearn.datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F5n8xWjYXpdH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X, y = mglearn.datasets.load_extended_boston()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "lr = LinearRegression().fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OlhvbDCGXpdK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "08769c0f-8550-446c-825f-a6731c06ddf9"
      },
      "cell_type": "code",
      "source": [
        "print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\n",
        "print(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score: 0.95\n",
            "Test set score: 0.61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nUi1_YjTXpdT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Clearly we are overfitting here.\n"
      ]
    },
    {
      "metadata": {
        "id": "oIU1GGGAXpdU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data13 = load_boston()\n",
        "X13, y13 = data13.data, data13.target\n",
        "\n",
        "X13_train, X13_test, y13_train, y13_test = train_test_split(X13, y13, random_state=0)\n",
        "lr = LinearRegression().fit(X13_train, y13_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L1LiAdTJXpdY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ba785193-b2c2-4032-fd2f-2d3ecbd31c26"
      },
      "cell_type": "code",
      "source": [
        "data13.DESCR"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "e6-RlmlTXpdf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f1bf5515-bdb3-4cb5-8694-348d93a7bb03"
      },
      "cell_type": "code",
      "source": [
        "print(\"Training set score ttt: {:.2f}\".format(lr.score(X13_train, y13_train)))\n",
        "print(\"Test set score: {:.2f}\".format(lr.score(X13_test, y13_test)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score ttt: 0.77\n",
            "Test set score: 0.64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zVldNtGJXpdj",
        "colab_type": "raw"
      },
      "cell_type": "markdown",
      "source": [
        "As we are considering only 13 features, training set also has less accuracy."
      ]
    },
    {
      "metadata": {
        "id": "VV-vFBHbXpdk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Regularization\n",
        "\n",
        "In supervised machine learning, models are trained on a subset of data aka training data. The goal is to compute the target of each training example from the training data.\n",
        "\n",
        "Now, overfitting happens when model learns signal as well as noise in the training data and wouldn’t perform well on new data on which model wasn’t trained on. In the example below, you can see underfitting in first few steps and overfitting in last few.\n",
        "\n",
        "Regularization basically adds the penalty as model complexity increases. Regularization parameter (lambda) penalizes all the parameters except intercept so that model generalizes the data and won’t overfit.\n",
        "\n",
        "A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression.\n",
        "\n",
        "The key difference between these two is the penalty term.\n",
        "\n",
        "Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function. \n",
        "\n",
        "However, if lambda is very large then it will add too much weight and it will lead to under-fitting. Having said that it’s important how lambda is chosen. This technique works very well to avoid over-fitting issue.\n",
        "\n",
        "Lasso Regression (Least Absolute Shrinkage and Selection Operator) adds “absolute value of magnitude” of coefficient as penalty term to the loss function.\n",
        "\n",
        "under-fit.\n",
        "\n",
        "The key difference between these techniques is that Lasso shrinks the less important feature’s coefficient to zero thus, removing some feature altogether. So, this works well for feature selection in case we have a huge number of features.\n",
        "\n",
        "Traditional methods like cross-validation, stepwise regression to handle overfitting and perform feature selection work well with a small set of features but these techniques are a great alternative when we are dealing with a large set of features."
      ]
    },
    {
      "metadata": {
        "id": "BwkvX-yVXpdm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Ridge Regression"
      ]
    },
    {
      "metadata": {
        "id": "OH9hXSXNXpdn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ridge regression is also a linear model for regression, so the formula it uses to make predictions is the same one used for ordinary least squares. In ridge regression, though, the coefficients (w) are chosen not only so that they predict well on the training data, but also to fit an additional constraint. We also want the magnitude of coefficients to be as small as possible; in other words, all entries of w should be close to zero. Intuitively, this means each feature should have as little effect on the outcome as possible (which translates to having a small slope), while still predicting well. This constraint is an example of what is called regularization. Regularization means explicitly restricting a model to avoid overfitting."
      ]
    },
    {
      "metadata": {
        "id": "7fXfTAtpXpdo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "ridge = Ridge().fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mfljopQCXpdr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "01ad23a0-a923-44e9-b9d0-aec1f9e09ea2"
      },
      "cell_type": "code",
      "source": [
        "ridge.predict(X_test)\n",
        "print(ridge.score(X_train, y_train))\n",
        "print(ridge.score(X_test, y_test))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8857966585170941\n",
            "0.7527683481744751\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-aqVPMIeXpdv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see, the training set score of Ridge is lower than for LinearRegression, while the test set score is higher. This is consistent with our expectation. With linear regression, we were overfitting our data. Ridge is a more restricted model, so we are less likely to overfit. A less complex model means worse performance on the training set, but better generalization. As we are only interested in generalization performance, we should choose the Ridge model over the LinearRegression model.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "CM91OxNyXpdw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Ridge model makes a trade-off between the simplicity of the model (near-zero coefficients) and its performance on the training set. How much importance the model places on simplicity versus training set performance can be specified by the user, using the alpha parameter. In the previous example, we used the default parameter alpha=1.0. There is no reason why this will give us the best trade-off, though. The optimum setting of alpha depends on the particular dataset we are using. Increasing alpha forces coefficients to move more toward zero, which decreases training set performance but might help generalization. For example:"
      ]
    },
    {
      "metadata": {
        "id": "vggCudVvXpdx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b3aaa716-936c-4baa-c7b1-5d2283050237"
      },
      "cell_type": "code",
      "source": [
        "ridge10 = Ridge(alpha=10).fit(X_train, y_train)\n",
        "print(\"Training set score: {:.2f}\".format(ridge10.score(X_train, y_train)))\n",
        "print(\"Test set score: {:.2f}\".format(ridge10.score(X_test, y_test)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score: 0.79\n",
            "Test set score: 0.64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AxArVYVuXpd4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Decreasing alpha allows the coefficients to be less restricted, meaning we move right in Figure 2-1. For very small values of alpha, coefficients are barely restricted at all, and we end up with a model that resembles LinearRegression"
      ]
    },
    {
      "metadata": {
        "id": "UlTve6kuXpd6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lasso Regression"
      ]
    },
    {
      "metadata": {
        "id": "1PN0P1O5Xpd8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "An alternative to Ridge for regularizing linear regression is Lasso. As with ridge regression, using the lasso also restricts coefficients to be close to zero, but in a slightly different way, called L1 regularization. The consequence of L1 regularization is that when using the lasso, some coefficients are exactly zero. This means some features are entirely ignored by the model. This can be seen as a form of automatic feature selection. Having some coefficients be exactly zero often makes a model easier to interpret, and can reveal the most important features of your model."
      ]
    },
    {
      "metadata": {
        "id": "UxW68xdPXpd9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "be8acb52-24e7-400a-f883-cf0139df8723"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "lasso = Lasso().fit(X_train, y_train)\n",
        "print(\"Training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\n",
        "print(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\n",
        "print(\"Number of features used: {}\".format(np.sum(lasso.coef_ != 0)))\n",
        "lasso.coef_"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score: 0.29\n",
            "Test set score: 0.21\n",
            "Number of features used: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.        ,  0.        , -0.        ,  0.        , -0.        ,\n",
              "        0.        , -0.        ,  0.        , -0.        , -0.        ,\n",
              "       -0.        ,  0.        , -5.3529079 , -0.        ,  0.        ,\n",
              "       -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
              "       -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
              "       -0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
              "       -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
              "       -0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "        0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
              "       -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
              "       -0.        , -0.        , -0.        ,  0.        ,  0.        ,\n",
              "        0.        , -0.        , -0.        , -0.        ,  0.        ,\n",
              "       -0.        , -0.        ,  0.        , -0.        , -1.05063037,\n",
              "       -3.3104274 , -0.        , -0.        ,  0.        , -0.        ,\n",
              "       -0.        , -0.        ,  0.        , -0.        , -0.41386744,\n",
              "       -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
              "       -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
              "       -0.        ,  0.        , -0.        , -0.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "t71HeN81XpeC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "21f65c33-fa40-4e2e-ee61-9ed4aa4d19c1"
      },
      "cell_type": "code",
      "source": [
        "# we increase the default setting of \"max_iter\",\n",
        "# otherwise the model would warn us that we should increase max_iter.\n",
        "lasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train)\n",
        "print(\"Training set score: {:.2f}\".format(lasso001.score(X_train, y_train)))\n",
        "print(\"Test set score: {:.2f}\".format(lasso001.score(X_test, y_test)))\n",
        "print(\"Number of features used: {}\".format(np.sum(lasso001.coef_ != 0)))\n",
        "lasso001.coef_"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set score: 0.90\n",
            "Test set score: 0.77\n",
            "Number of features used: 33\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -0.        ,  -0.        ,  -0.        ,   0.        ,\n",
              "        -0.        ,   0.        ,  -0.        ,  -1.43260465,\n",
              "        10.94771183,   0.        ,   0.        ,   0.        ,\n",
              "        -0.39260971,  -0.        ,  -0.        ,  -0.        ,\n",
              "         0.        ,  -0.        ,  -0.        ,  -0.        ,\n",
              "        -0.        ,  -8.75628457,  -0.        ,  -0.        ,\n",
              "        -0.        ,  -0.        ,   2.10027365,  -0.        ,\n",
              "         0.        ,  -0.        ,   0.        ,  -0.        ,\n",
              "         0.        ,   0.        ,  -0.        ,   0.        ,\n",
              "        -0.        ,  -0.        ,   0.        ,   0.        ,\n",
              "         0.        ,  -0.        ,   0.        ,  -3.96958293,\n",
              "         0.        ,   6.61845793,  -0.        ,  -0.        ,\n",
              "        -0.        ,   0.        ,  -4.42086828,  -2.10371434,\n",
              "         3.79607992,  -0.        ,   4.38591262,   0.        ,\n",
              "         0.        ,   0.1795777 ,  -0.        ,  -1.1614282 ,\n",
              "        -4.33485764,  -0.        ,  -0.        ,  -2.13549022,\n",
              "        -0.        ,  -1.85967636,  -0.        ,  -0.        ,\n",
              "        29.81957225,  -2.05624806,   0.        , -11.98034348,\n",
              "       -11.14870694, -11.67147204,  12.974385  , -10.93053676,\n",
              "        -0.        ,  -0.        ,   3.441574  ,   0.        ,\n",
              "        -0.        ,  -0.        ,  -8.56750394,   0.        ,\n",
              "         0.        ,  -0.        ,   0.        ,  -7.26115293,\n",
              "        -0.        ,   0.        ,   0.94254631,   0.        ,\n",
              "         0.        ,  -7.62603559,   1.65255854,   0.        ,\n",
              "         0.        , -17.39462971,   0.        ,   0.        ,\n",
              "        -0.        ,   0.34404124,  -8.24645566,  17.5606721 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "kPIu32SKXpeG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A lower alpha allowed us to fit a more complex model, which worked better on the training and test data. The performance is slightly better than using Ridge, and we are using only 33 of the 105 features. This makes this model potentially easier to understand.\n",
        "\n",
        "If we set alpha too low, however, we again remove the effect of regularization and end up overfitting, with a result similar to LinearRegression"
      ]
    },
    {
      "metadata": {
        "id": "mjudJRmHXpeH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note : \n",
        "FIT RESETS A MODEL\n",
        "An important property of scikit-learn models is that calling fit will always reset everything a model previously learned. So if you build a model on one dataset, and then call fit again on a different dataset, the model will “forget” everything it learned from the first dataset. You can call fit as often as you like on a model, and the outcome will be the same as calling fit on a “new” model."
      ]
    },
    {
      "metadata": {
        "id": "6ENHCOygXpeI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c"
      ]
    },
    {
      "metadata": {
        "id": "-V5jkC1eXpeJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}